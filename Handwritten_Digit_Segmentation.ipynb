{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "T-XO9xkN1OR3",
        "e2ueOTmc3HxJ",
        "Ux5nAU155E5O"
      ],
      "gpuType": "T4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6x_sGPQocpw"
      },
      "source": [
        "# Image Segmentation of Handwritten Digits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3g9dJxSxmN"
      },
      "source": [
        "## Imports\n",
        "\n",
        "As usual, let's start by importing the packages you will use in this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aifz2907kxYN"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "yUWEWDuVEnBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROok0i9rMcu0"
      },
      "source": [
        "# find and extract to a local folder ('/tmp/training')\n",
        "local_zip = '/content/drive/MyDrive/m2nist.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/training')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy17LYR7XJNa"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy_pw5I2-xLP"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def read_image_and_annotation(image, annotation):\n",
        "  '''\n",
        "  Casts the image and annotation to their expected data type and\n",
        "  normalizes the input image so that each pixel is in the range [-1, 1]\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- input image\n",
        "    annotation (numpy array) -- ground truth label map\n",
        "\n",
        "  Returns:\n",
        "    preprocessed image-annotation pair\n",
        "  '''\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.reshape(image, (image.shape[0], image.shape[1], 1,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  image = image / 127.5\n",
        "  image -= 1\n",
        "\n",
        "  return image, annotation\n",
        "\n",
        "\n",
        "def get_training_dataset(images, annos):\n",
        "  '''\n",
        "  Prepares shuffled batches of the training set.\n",
        "\n",
        "  Args:\n",
        "    images (list of strings) -- paths to each image file in the train set\n",
        "    annos (list of strings) -- paths to each label map in the train set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed train set\n",
        "  '''\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  training_dataset = training_dataset.map(read_image_and_annotation)\n",
        "\n",
        "  training_dataset = training_dataset.shuffle(512, reshuffle_each_iteration=True)\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  training_dataset = training_dataset.prefetch(-1)\n",
        "\n",
        "  return training_dataset\n",
        "\n",
        "\n",
        "def get_validation_dataset(images, annos):\n",
        "  '''\n",
        "  Prepares batches of the validation set.\n",
        "\n",
        "  Args:\n",
        "    images (list of strings) -- paths to each image file in the val set\n",
        "    annos (list of strings) -- paths to each label map in the val set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed validation set\n",
        "  '''\n",
        "  validation_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  validation_dataset = validation_dataset.map(read_image_and_annotation)\n",
        "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()\n",
        "\n",
        "  return validation_dataset\n",
        "\n",
        "\n",
        "def get_test_dataset(images, annos):\n",
        "  '''\n",
        "  Prepares batches of the test set.\n",
        "\n",
        "  Args:\n",
        "    images (list of strings) -- paths to each image file in the test set\n",
        "    annos (list of strings) -- paths to each label map in the test set\n",
        "\n",
        "  Returns:\n",
        "    tf Dataset containing the preprocessed validation set\n",
        "  '''\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  test_dataset = test_dataset.map(read_image_and_annotation)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return test_dataset\n",
        "\n",
        "\n",
        "def load_images_and_segments():\n",
        "  '''\n",
        "  Loads the images and segments as numpy arrays from npy files\n",
        "  and makes splits for training, validation and test datasets.\n",
        "\n",
        "  Returns:\n",
        "    3 tuples containing the train, val, and test splits\n",
        "  '''\n",
        "\n",
        "  #Loads images and segmentation masks.\n",
        "  images = np.load('/tmp/training/combined.npy')\n",
        "  segments = np.load('/tmp/training/segmented.npy')\n",
        "\n",
        "  #Makes training, validation, test splits from loaded images and segmentation masks.\n",
        "  train_images, val_images, train_annos, val_annos = train_test_split(images, segments, test_size=0.2, shuffle=True)\n",
        "  val_images, test_images, val_annos, test_annos = train_test_split(val_images, val_annos, test_size=0.2, shuffle=True)\n",
        "\n",
        "  return (train_images, train_annos), (val_images, val_annos), (test_images, test_annos)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIS70_um_Y7n",
        "lines_to_next_cell": 2
      },
      "source": [
        "# Load Dataset\n",
        "train_slices, val_slices, test_slices = load_images_and_segments()\n",
        "\n",
        "# Create training, validation, test datasets.\n",
        "training_dataset = get_training_dataset(train_slices[0], train_slices[1])\n",
        "validation_dataset = get_validation_dataset(val_slices[0], val_slices[1])\n",
        "test_dataset = get_test_dataset(test_slices[0], test_slices[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "d46YCbvPafbp"
      },
      "source": [
        "# Visualization Utilities\n",
        "\n",
        "# there are 11 classes in the dataset: one class for each digit (0 to 9) plus the background class\n",
        "n_classes = 11\n",
        "\n",
        "# assign a random color for each class\n",
        "colors = [tuple(np.random.randint(256, size=3) / 255.0) for i in range(n_classes)]\n",
        "\n",
        "def fuse_with_pil(images):\n",
        "  '''\n",
        "  Creates a blank image and pastes input images\n",
        "\n",
        "  Args:\n",
        "    images (list of numpy arrays) - numpy array representations of the images to paste\n",
        "\n",
        "  Returns:\n",
        "    PIL Image object containing the images\n",
        "  '''\n",
        "\n",
        "  widths = (image.shape[1] for image in images)\n",
        "  heights = (image.shape[0] for image in images)\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    pil_image = PIL.Image.fromarray(np.uint8(im))\n",
        "    new_im.paste(pil_image, (x_offset,0))\n",
        "    x_offset += im.shape[1]\n",
        "\n",
        "  return new_im\n",
        "\n",
        "\n",
        "def give_color_to_annotation(annotation):\n",
        "  '''\n",
        "  Converts a 2-D annotation to a numpy array with shape (height, width, 3) where\n",
        "  the third axis represents the color channel. The label values are multiplied by\n",
        "  255 and placed in this axis to give color to the annotation\n",
        "\n",
        "  Args:\n",
        "    annotation (numpy array) - label map array\n",
        "\n",
        "  Returns:\n",
        "    the annotation array with an additional color channel/axis\n",
        "  '''\n",
        "  seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
        "\n",
        "  for c in range(n_classes):\n",
        "    segc = (annotation == c)\n",
        "    seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
        "    seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
        "    seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
        "\n",
        "  return seg_img\n",
        "\n",
        "\n",
        "def show_annotation_and_prediction(image, annotation, prediction, iou_list, dice_score_list):\n",
        "  '''\n",
        "  Displays the images with the ground truth and predicted label maps. Also overlays the metrics.\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- the input image\n",
        "    annotation (numpy array) -- the ground truth label map\n",
        "    prediction (numpy array) -- the predicted label map\n",
        "    iou_list (list of floats) -- the IOU values for each class\n",
        "    dice_score_list (list of floats) -- the Dice Score for each class\n",
        "  '''\n",
        "\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  true_img = give_color_to_annotation(new_ann)\n",
        "  pred_img = give_color_to_annotation(prediction)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "  image = np.uint8(image)\n",
        "  images = [image, np.uint8(pred_img), np.uint8(true_img)]\n",
        "\n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0 and idx < 10]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "\n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\".join(display_string_list)\n",
        "\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  for idx, im in enumerate(images):\n",
        "    plt.subplot(1, 3, idx+1)\n",
        "    if idx == 1:\n",
        "      plt.xlabel(display_string)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(im)\n",
        "\n",
        "\n",
        "def show_annotation_and_image(image, annotation):\n",
        "  '''\n",
        "  Displays the image and its annotation side by side\n",
        "\n",
        "  Args:\n",
        "    image (numpy array) -- the input image\n",
        "    annotation (numpy array) -- the label map\n",
        "  '''\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  seg_img = give_color_to_annotation(new_ann)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "\n",
        "  image = np.uint8(image)\n",
        "  images = [image, seg_img]\n",
        "\n",
        "  images = [image, seg_img]\n",
        "  fused_img = fuse_with_pil(images)\n",
        "  plt.imshow(fused_img)\n",
        "\n",
        "\n",
        "def list_show_annotation(dataset, num_images):\n",
        "  '''\n",
        "  Displays images and its annotations side by side\n",
        "\n",
        "  Args:\n",
        "    dataset (tf Dataset) -- batch of images and annotations\n",
        "    num_images (int) -- number of images to display\n",
        "  '''\n",
        "  ds = dataset.unbatch()\n",
        "\n",
        "  plt.figure(figsize=(20, 15))\n",
        "  plt.title(\"Images And Annotations\")\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
        "\n",
        "  for idx, (image, annotation) in enumerate(ds.take(num_images)):\n",
        "    plt.subplot(5, 5, idx + 1)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    show_annotation_and_image(image.numpy(), annotation.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFO_hIhLWYT4"
      },
      "source": [
        "# get 25 images from the training set\n",
        "list_show_annotation(training_dataset, 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdgVkp8wZua0"
      },
      "source": [
        "# get 25 images from the validation set\n",
        "list_show_annotation(validation_dataset, 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv2k8xabRb8"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EJEf484312h",
        "lines_to_next_cell": 2
      },
      "source": [
        "# start the encoder using the default input size 64 x 84\n",
        "convs, img_input = FCN8()\n",
        "\n",
        "# pass the convolutions obtained in the encoder to the decoder\n",
        "dec_op = fcn8_decoder(convs, n_classes)\n",
        "\n",
        "# define the model specifying the input (batch of images) and output (decoder output)\n",
        "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GAenp1M4gXx"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAAXygZtbZmu"
      },
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpWpp8h4g_rE"
      },
      "source": [
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510v0aVDXv1f"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HoZwpGWhMB-"
      },
      "source": [
        "# OTHER THAN SETTING THE EPOCHS NUMBER, DO NOT CHANGE ANY OTHER CODE\n",
        "\n",
        "EPOCHS = 70\n",
        "\n",
        "steps_per_epoch = 4000//BATCH_SIZE\n",
        "validation_steps = 800//BATCH_SIZE\n",
        "test_steps = 200//BATCH_SIZE\n",
        "\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih-Q7GoXzJe"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTkaFM2X1gr"
      },
      "source": [
        "### Make Predictions\n",
        "\n",
        "Let's get the predictions using our test dataset as input and print the shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zENjQuK0luH5"
      },
      "source": [
        "results = model.predict(test_dataset, steps=test_steps)\n",
        "\n",
        "print(results.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IReKPT_DZHjA"
      },
      "source": [
        "As you can see, the resulting shape is `(192, 64, 84, 11)`. This means that for each of the 192 images that we have in our test set, there are 11 predictions generated (i.e. one for each class: 0 to 1 plus background)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBPBqnHyaSaG"
      },
      "source": [
        "Thus, if you want to see the *probability* of the upper leftmost pixel of the 1st image belonging to class 0, then you can print something like `results[0,0,0,0]`. If you want the probability of the same pixel at class 10, then do `results[0,0,0,10]`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwFiR9WAf0Av"
      },
      "source": [
        "print(results[0,0,0,0])\n",
        "print(results[0,0,0,10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Uj_uuV9TQt"
      },
      "source": [
        "results = np.argmax(results, axis=3)\n",
        "\n",
        "print(results.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBeBwvHQd2pZ"
      },
      "source": [
        "print(results[0,0,0])\n",
        "\n",
        "# prediction map for image 0\n",
        "print(results[0,:,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpKDUuAWX5Pj"
      },
      "source": [
        "### Metrics\n",
        "\n",
        "We showed in the lectures two ways to evaluate your predictions. The *intersection over union (IOU)* and the *dice score*. Recall that:\n",
        "\n",
        "$$IOU = \\frac{area\\_of\\_overlap}{area\\_of\\_union}$$\n",
        "<br>\n",
        "$$Dice Score = 2 * \\frac{area\\_of\\_overlap}{combined\\_area}$$\n",
        "\n",
        "The code below does that for you as you've also seen in the ungraded lab. A small smoothing factor is introduced in the denominators to prevent possible division by zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKTpLmly_RXb"
      },
      "source": [
        "def class_wise_metrics(y_true, y_pred):\n",
        "  '''\n",
        "  Computes the class-wise IOU and Dice Score.\n",
        "\n",
        "  Args:\n",
        "    y_true (tensor) - ground truth label maps\n",
        "    y_pred (tensor) - predicted label maps\n",
        "  '''\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothing_factor = 0.00001\n",
        "\n",
        "  for i in range(n_classes):\n",
        "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "\n",
        "    iou = (intersection) / (combined_area - intersection + smoothing_factor)\n",
        "    class_wise_iou.append(iou)\n",
        "\n",
        "    dice_score =  2 * ((intersection) / (combined_area + smoothing_factor))\n",
        "    class_wise_dice_score.append(dice_score)\n",
        "\n",
        "  return class_wise_iou, class_wise_dice_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWPwM4ZhHjE"
      },
      "source": [
        "### Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkbsk_P1fpRM",
        "lines_to_next_cell": 2
      },
      "source": [
        "# place a number here between 0 to 191 to pick an image from the test set\n",
        "integer_slider = 105\n",
        "\n",
        "ds = test_dataset.unbatch()\n",
        "ds = ds.batch(200)\n",
        "images = []\n",
        "\n",
        "y_true_segments = []\n",
        "for image, annotation in ds.take(2):\n",
        "  y_true_segments = annotation\n",
        "  images = image\n",
        "\n",
        "\n",
        "iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=2), results[integer_slider])\n",
        "show_annotation_and_prediction(image[integer_slider], annotation[integer_slider], results[integer_slider], iou, dice_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiG9K4t6X9iZ"
      },
      "source": [
        "### Compute IOU Score and Dice Score of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2706boF0CNNS",
        "lines_to_next_cell": 2
      },
      "source": [
        "cls_wise_iou, cls_wise_dice_score = class_wise_metrics(np.argmax(y_true_segments, axis=3), results)\n",
        "\n",
        "average_iou = 0.0\n",
        "for idx, (iou, dice_score) in enumerate(zip(cls_wise_iou[:-1], cls_wise_dice_score[:-1])):\n",
        "  print(\"Digit {}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score))\n",
        "  average_iou += iou\n",
        "\n",
        "grade = average_iou * 10\n",
        "\n",
        "print(\"\\nGrade is \" + str(grade))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvw0HLY2kV3w"
      },
      "source": [
        "## Save the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULCfGHEKkaO0"
      },
      "source": [
        "model.save(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}